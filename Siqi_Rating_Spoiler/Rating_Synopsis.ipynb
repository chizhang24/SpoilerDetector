{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6461e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3871a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# Download NLTK stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823298d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82044de9",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606b638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'IMDB_movie_details.json'\n",
    "data = []\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051eb1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorize ratings\n",
    "df['rating_class'] = np.where(df['rating'].astype(float) >= 8, 2,\n",
    "                 np.where(df['rating'].astype(float) <= 6, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a12b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class_0 = df[df['rating_class'] == 0]\n",
    "df_class_1 = df[df['rating_class'] == 1]\n",
    "df_class_2 = df[df['rating_class'] == 2]\n",
    "print(df['rating_class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0c8f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['rating_class'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['rating_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472c9797",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt = train_df.plot_synopsis.tolist()\n",
    "train_label = train_df.rating_class.tolist()\n",
    "\n",
    "val_txt = val_df.plot_synopsis.tolist()\n",
    "val_label = val_df.rating_class.tolist()\n",
    "\n",
    "test_txt = test_df.plot_synopsis.tolist()\n",
    "test_label = test_df.rating_class.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca119acf",
   "metadata": {},
   "source": [
    "## Set up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a34188",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification # \n",
    "\n",
    "model_path = \"allenai/longformer-base-4096\"\n",
    "model_name = 'longformer'\n",
    "context_len = 4096\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=len(set(train_label))).to(f'cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45be8031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import *\n",
    "\n",
    "# tokenize input text\n",
    "# load preprocessed results if the specified path exists\n",
    "new_train_id = down_sample(train_label)\n",
    "print(len(new_train_id))\n",
    "train_data = create_dataset([train_txt[i] for i in new_train_id], [train_label[i] for i in new_train_id], tokenizer, f'syno_{model_name}_{context_len}_train.pt', max_len=context_len, num_cpus=8)\n",
    "val_data = create_dataset(val_txt, val_label, tokenizer, f'syno_{model_name}_{context_len}_val.pt', max_len=context_len, num_cpus=8)\n",
    "test_data = create_dataset(test_txt, test_label, tokenizer, f'syno_{model_name}_{context_len}_test.pt',max_len=context_len, num_cpus=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77ce9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples_ids = up_sample(train_label)\n",
    "train_loader = make_dataloader(train_data, 4, shuffle=True)\n",
    "val_loader = make_dataloader(val_data, 16)\n",
    "test_loader = make_dataloader(test_data, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6995a47",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8876e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup, get_constant_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8e280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 10\n",
    "total_steps = len(train_loader) * max_epochs\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5, eps=1e-8)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1*total_steps,\n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "val_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bec579",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.zero_grad()\n",
    "best_score = 0\n",
    "for e in range(max_epochs):\n",
    "    print(f'Training epoch {e+2}')\n",
    "    total_train_loss = 0\n",
    "    for step, batch in enumerate(tqdm(train_loader)):\n",
    "        input_ids = batch[0].to(f'cuda:0')\n",
    "        input_mask = batch[1].to(f'cuda:0')\n",
    "        labels = batch[2].to(f'cuda:0')\n",
    "        logits = model(input_ids, \n",
    "                    attention_mask=input_mask).logits\n",
    "        loss = loss_fn(logits, labels)\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        model.zero_grad()\n",
    "        if (step+1) % val_step == 0:\n",
    "            class_pred = []\n",
    "            labels = []\n",
    "            for batch in tqdm(val_loader):\n",
    "                batch_max_len = batch[1].sum(dim=1).max()\n",
    "                input_ids = batch[0][:, :batch_max_len].to(f'cuda:0')\n",
    "                input_mask = batch[1][:, :batch_max_len].to(f'cuda:0')\n",
    "                with torch.no_grad():\n",
    "                    logits = model(input_ids,  \n",
    "                               attention_mask=input_mask).logits\n",
    "                    preds = logits.argmax(dim=-1)\n",
    "                    class_pred.extend(preds.cpu().numpy().tolist())\n",
    "                    labels.extend(batch[2].numpy().tolist())\n",
    "            micro, macro = acc(class_pred, labels)\n",
    "            print(f'Micro F1: {micro}, Macro F1: {macro}')\n",
    "            if micro > best_score:\n",
    "                best_score = micro\n",
    "                torch.save(model.state_dict(), 'syno_best_val_model.pt')\n",
    "    torch.save(model.state_dict(), f'syno_epoch_{e}_model.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed436ef",
   "metadata": {},
   "source": [
    "## Load and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1081efcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "\n",
    "model.load_state_dict(torch.load('best_val_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d547ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import *\n",
    "\n",
    "# test on balanced data\n",
    "new_test_id = down_sample(test_label)\n",
    "print(len(new_test_id))\n",
    "sampled_test_data = create_dataset([test_txt[i] for i in new_test_id], [test_label[i] for i in new_test_id], tokenizer, 'down_plot_syno_long_sampled_test.pt', max_len=4096, num_cpus=8)\n",
    "\n",
    "test_loader = make_dataloader(sampled_test_data, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783b8c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_pred = []\n",
    "labels = []\n",
    "for batch in tqdm(test_loader):\n",
    "    batch_max_len = batch[1].sum(dim=1).max()\n",
    "    input_ids = batch[0][:, :batch_max_len].to(f'cuda:0')\n",
    "    input_mask = batch[1][:, :batch_max_len].to(f'cuda:0')\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids,  \n",
    "                   attention_mask=input_mask).logits\n",
    "        \n",
    "        preds = logits.argmax(dim=-1)\n",
    "        class_pred.extend(preds.cpu().numpy().tolist())\n",
    "        labels.extend(batch[2].numpy().tolist())\n",
    "print(acc(class_pred, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f4a5a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trf",
   "language": "python",
   "name": "trf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
