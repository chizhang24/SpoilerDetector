{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6461e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3871a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# Download NLTK stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823298d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c1391e",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606b638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/IMDB_reviews.json'\n",
    "data = []\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f3a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'www\\S+', '', text)\n",
    "    \n",
    "    # Remove emails\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
    "    \n",
    "    # Normalize whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text\n",
    "# Set up tqdm for pandas apply\n",
    "tqdm.pandas(desc=\"Cleaning Text\")\n",
    "\n",
    "# Apply the cleaning function with a progress bar\n",
    "df['cleaned_review_text'] = df['review_text'].progress_apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ada9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path where you want to save the JSON file\n",
    "json_file_path = '../data/cleandata.json'  # Change this to your desired file path\n",
    "\n",
    "# Save the DataFrame to a JSON file\n",
    "df.to_json(json_file_path, orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655e78f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprocessed data\n",
    "file_path = '../data/cleandata.json'\n",
    "data = []\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0c8f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['is_spoiler'])\n",
    "# Split the data \n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472c9797",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt = train_df.cleaned_review_text.tolist()\n",
    "train_label = train_df.label.tolist()\n",
    "\n",
    "val_txt = val_df.cleaned_review_text.tolist()\n",
    "val_label = val_df.label.tolist()\n",
    "\n",
    "test_txt = test_df.cleaned_review_text.tolist()\n",
    "test_label = test_df.label.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616c991e",
   "metadata": {},
   "source": [
    "## Set up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a34188",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification # \n",
    "\n",
    "model_path = \"google-bert/bert-base-uncased\"  # \"allenai/longformer-base-4096\"\n",
    "model_name = 'bert'\n",
    "context_len = 512   # 4096\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=len(set(train_label))).to(f'cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45be8031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import *\n",
    "\n",
    "# tokenize input text\n",
    "# load preprocessed results if the specified path exists\n",
    "new_train_id = down_sample(train_label)\n",
    "print('Numer of training samples', len(new_train_id))\n",
    "train_data = create_dataset([train_txt[i] for i in new_train_id], [train_label[i] for i in new_train_id], tokenizer, f'review_{model_name}_{context_len}_train.pt', max_len=context_len, num_cpus=8)\n",
    "val_data = create_dataset(val_txt, val_label, tokenizer, f'review_{model_name}_{context_len}_val.pt', max_len=context_len, num_cpus=8)\n",
    "test_data = create_dataset(test_txt, test_label, tokenizer, f'review_{model_name}_{context_len}_test.pt',max_len=context_len, num_cpus=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77ce9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = make_dataloader(train_data, 16, shuffle=True)\n",
    "val_loader = make_dataloader(val_data, 32)\n",
    "test_loader = make_dataloader(test_data, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7250ed2",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8876e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup, get_constant_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fa688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 10\n",
    "total_steps = len(train_loader) * max_epochs\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5, eps=1e-8)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1*total_steps,\n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "val_step = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55c23bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()\n",
    "best_score = 0\n",
    "for e in range(max_epochs):\n",
    "    print(f'Training epoch {e+2}')\n",
    "    total_train_loss = 0\n",
    "    for step, batch in enumerate(tqdm(train_loader)):\n",
    "        input_ids = batch[0].to(f'cuda:0')\n",
    "        input_mask = batch[1].to(f'cuda:0')\n",
    "        labels = batch[2].to(f'cuda:0')\n",
    "        logits = model(input_ids, \n",
    "                    attention_mask=input_mask).logits\n",
    "        loss = loss_fn(logits, labels)\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        model.zero_grad()\n",
    "        if (step+1) % val_step == 0:\n",
    "            class_pred = []\n",
    "            labels = []\n",
    "            for batch in tqdm(val_loader):\n",
    "                batch_max_len = batch[1].sum(dim=1).max()\n",
    "                input_ids = batch[0][:, :batch_max_len].to(f'cuda:0')\n",
    "                input_mask = batch[1][:, :batch_max_len].to(f'cuda:0')\n",
    "                with torch.no_grad():\n",
    "                    logits = model(input_ids,  \n",
    "                               attention_mask=input_mask).logits\n",
    "                    preds = logits.argmax(dim=-1)\n",
    "                    class_pred.extend(preds.cpu().numpy().tolist())\n",
    "                    labels.extend(batch[2].numpy().tolist())\n",
    "            micro, macro = acc(class_pred, labels)\n",
    "            print(f'Micro F1: {micro}, Macro F1: {macro}')\n",
    "            if micro > best_score:\n",
    "                best_score = micro\n",
    "                torch.save(model.state_dict(), 'best_val_model.pt')\n",
    "    torch.save(model.state_dict(), f'epoch_{e}_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b445aef9",
   "metadata": {},
   "source": [
    "## Load and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1585b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "\n",
    "model.load_state_dict(torch.load('best_val_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c5ee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_pred = []\n",
    "labels = []\n",
    "for batch in tqdm(test_loader):\n",
    "    batch_max_len = batch[1].sum(dim=1).max()\n",
    "    input_ids = batch[0][:, :batch_max_len].to(f'cuda:0')\n",
    "    input_mask = batch[1][:, :batch_max_len].to(f'cuda:0')\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids,  \n",
    "                   attention_mask=input_mask).logits\n",
    "        \n",
    "        preds = logits.argmax(dim=-1)\n",
    "        class_pred.extend(preds.cpu().numpy().tolist())\n",
    "        labels.extend(batch[2].numpy().tolist())\n",
    "print(acc(class_pred, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e35e879",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trf",
   "language": "python",
   "name": "trf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
